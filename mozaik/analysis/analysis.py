"""
This module contains the Mozaik analysis interface and implementation of
various analysis algorithms
"""

import numpy
import scipy
import time
import quantities as qt
import mozaik.tools.units as munits
from mozaik.tools.mozaik_parametrized import colapse, colapse_to_dictionary, MozaikParametrized
from mozaik.analysis.analysis_data_structures import PerNeuronValue, \
                                        ConductanceSignalList, AnalogSignalList
from mozaik.analysis.analysis_helper_functions import psth
from mozaik.framework.interfaces import MozaikParametrizeObject
from NeuroTools.parameters import ParameterSet
from mozaik.storage import queries
from neo.core.analogsignal import AnalogSignal
from mozaik.tools.circ_stat import circ_mean, circular_dist
from mozaik.tools.neo_object_operations import neo_mean
import mozaik

logger = mozaik.getMozaikLogger("Mozaik")


class Analysis(MozaikParametrizeObject):
    """
    Analysis encapsulates analysis algorithms.

    The interface is extremely simple: it only requires the implementation of
    the `perform_analysis` function which when called performs the analysis.
    This function should retrieve its own data from the `DataStoreView` that is
    supplied in the `datastore` parameter. Further, it should include `tags`
    as the tags for all `AnalysisDataStructure` objects that it creates.
    (See the description of the `AnalysisDataStructure.tags` attribute.

    Arguments:
        datastore (DataStoreView): the datastore from which to pull data.
        parameters (ParameterSet): the parameter set
        tags (list(str)): tags to attach to the AnalysisDataStructures
                          generated by the analysis

    """

    def __init__(self, datastore, parameters, tags=None):
        MozaikParametrizeObject.__init__(self, parameters)
        self.datastore = datastore
        if tags == None:
            self.tags = []
        else:
            self.tags = tags

    def analyse(self):
        t1 = time.time()
        logger.info('Starting ' + self.__class__.__name__ + ' analysis')
        self.perform_analysis()
        t2 = time.time()
        logger.warning(self.__class__.__name__ + ' analysis took: '
                       + str(t2-t1) + 'seconds')

    def perform_analysis(self):
        """
        The function that implements the analysis
        """
        raise NotImplementedError


class TrialAveragedFiringRate(Analysis):
    """
    This analysis takes all recordings with a
    `FullfieldDriftingSinusoidalGrating` stimulus. It averages over the
    trials and creates tuning curves with respect to the orientation
    parameter. Thus for each combination of the other stimulus parameters
    a tuning curve is created.
    """

    required_parameters = ParameterSet({
      'stimulus_type': str,  # The stimulus type for which to compute AveragedTuning
    })

    def perform_analysis(self):
        
        for sheet in self.datastore.sheets():
            dsv1 = queries.param_filter_query(self.datastore, sheet_name=sheet,st_name=self.parameters.stimulus_type)
            segs = dsv1.get_segments()
            st = [MozaikParametrized.idd(s) for s in dsv1.get_stimuli()]
            # transform spike trains due to stimuly to mean_rates
            mean_rates = [numpy.array(s.mean_rates()) for s in segs]
            # collapse against all parameters other then trial
            (mean_rates, s) = colapse(mean_rates, st, parameter_list=['trial'])
            # take a sum of each
            mean_rates = [sum(a)/len(a) for a in mean_rates]

            #JAHACK make sure that mean_rates() return spikes per second
            units = munits.spike / qt.s
            logger.debug('Adding PerNeuronValue containing trial averaged '
                         'firing rates to datastore')
            for mr, st in zip(mean_rates, s):
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(mr, units,
                                   stimulus_id=str(st),
                                   value_name='Firing rate',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   analysis_algorithm=self.__class__.__name__,
                                   period=None))


class PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage(Analysis):
    """
    This analysis takes a list of PerNeuronValues and a periodic parameter
    `parameter_name`.

    All PerNeuronValues have to belong to stimuli of the same type and
    contain the same type of values (i.e. have the same `value_name`).

    For each combination of parameters of the stimuli other than `parameter_name`
    `PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage` creates a
    PerNeuronValue which corresponsd to the vector average through the
    periodic domain of `parameter_name`.
    """

    required_parameters = ParameterSet({
        'parameter_name': str,  # The name of the parameter through which to calculate the VectorAverage
    })

    def perform_analysis(self):
        for sheet in self.datastore.sheets():
            # Get PerNeuronValue ASD and make sure they are all associated
            # with the same stimulus and do not differ in any
            # ASD parameters except the stimulus
            dsv = queries.param_filter_query(self.datastore, sheet_name=sheet,identifier='PerNeuronValue')
            if len(dsv.analysis_results) == 0:
                break
            assert queries.equal_ads_except(dsv, ['stimulus_id'])
            assert queries.ads_with_equal_stimulus_type(dsv, allow_None=True)
            assert len(set([dsv.value_name for a in dsv.get_analysis_result(sheet_name=sheet)])) , "PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage accepts only DSVs containing values with the same value_name"
            self.pnvs = dsv.get_analysis_result(sheet_name=sheet)
            # get stimuli
            st = [MozaikParametrized.idd(s.stimulus_id) for s in self.pnvs]

            d = colapse_to_dictionary([z.values for z in self.pnvs],
                                      st,
                                      self.parameters.parameter_name)
            for k in d.keys():
                keys, values = d[k]
                y = []
                x = []
                for v, p in zip(values, keys):
                    y.append(v)
                    x.append(numpy.zeros(numpy.shape(v)) + p)
                pref, sel = circ_mean(numpy.array(x),
                                      weights=numpy.array(y),
                                      axis=0,
                                      low=0,
                                      high=st[0].params()[self.parameters.parameter_name].period,
                                      normalize=True)

                logger.debug('PeriodicTuningCurvePreferenceAndSelectivity_VectorAverage: Adding PerNeuronValue to datastore')
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(pref,
                                   st[0].params()[self.parameters.parameter_name].units,
                                   value_name=self.parameters.parameter_name + ' preference',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   period=st[0].params()[self.parameters.parameter_name].period,
                                   analysis_algorithm=self.__class__.__name__,
                                   stimulus_id=str(k)))
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(sel,
                                   st[0].params()[self.parameters.parameter_name].units,
                                   value_name=self.parameters.parameter_name + ' selectivity',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   period=1.0,
                                   analysis_algorithm=self.__class__.__name__,
                                   stimulus_id=str(k)))


class GSTA(Analysis):
    """
    Computes conductance spike triggered average.

    Note that it does not assume that spikes are aligned with the conductance
    sampling rate and will pick the bin in which the given spike falls
    (within the conductance sampling rate binning) as the center of the
    conductance vector that is included in the STA.
    """

    required_parameters = ParameterSet({
        'length': float,  # length (in ms time) how long before and after spike to compute the GSTA
                          # it will be rounded down to fit the sampling frequency
        'neurons': list,  # the list of neuron indexes for which to compute the
    })

    def perform_analysis(self):
        dsv = self.datastore
        for sheet in dsv.sheets():
            dsv1 = queries.param_filter_query(dsv, sheet_name=sheet)
            st = dsv1.get_stimuli()
            segs = dsv1.get_segments()

            asl_e = []
            asl_i = []
            for n in self.parameters.neurons:
                sp = [s.spiketrains[n] for s in segs]
                g_e = [s.get_esyn(n) for s in segs]
                g_i = [s.get_isyn(n) for s in segs]
                asl_e.append(self.do_gsta(g_e, sp))
                asl_i.append(self.do_gsta(g_i, sp))
            self.datastore.full_datastore.add_analysis_result(
                ConductanceSignalList(asl_e,
                                      asl_i,
                                      self.parameters.neurons,
                                      sheet_name=sheet,
                                      tags=self.tags,
                                      analysis_algorithm=self.__class__.__name__))

    def do_gsta(self, analog_signal, sp):
        dt = analog_signal[0].sampling_period
        gstal = int(self.parameters.length/dt)
        gsta = numpy.zeros(2*gstal + 1,)
        count = 0
        for (ans, spike) in zip(analog_signal, sp):
            for time in spike:
                if time > ans.t_start  and time < ans.t_stop:
                    idx = int((time - ans.t_start)/dt)
                    if idx - gstal > 0 and (idx + gstal + 1) <= len(ans):
                        gsta = gsta + ans[idx-gstal:idx+gstal+1].flatten().magnitude
                        count +=1
        if count == 0:
            count = 1
        gsta = gsta / count
        gsta = gsta * analog_signal[0].units

        return AnalogSignal(gsta,
                            t_start=-gstal*dt,
                            sampling_period=dt,
                            units=analog_signal[0].units)


class Precision(Analysis):
    """
    Computes the precision as the autocorrelation between the PSTH of
    different trials.

    Takes all the responses in the datastore.
    """

    required_parameters = ParameterSet({
        'neurons': list,  # the list of neuron indexes for which to compute the
        'bin_length': float,  # (ms) the size of bin to construct the PSTH from
    })

    def perform_analysis(self):
        for sheet in self.datastore.sheets():
            # Load up spike trains for the right sheet and the corresponding
            # stimuli, and transform spike trains into psth
            dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
            psths = [psth(seg.spiketrains, self.parameters.bin_length)
                     for seg in dsv.get_segments()]

            st = [MozaikParametrized.idd(s) for s in dsv.get_stimuli()]

            # average across trials
            psths, stids = colapse(psths,
                                   st,
                                   parameter_list=['trial'],
                                   func=neo_mean,
                                   allow_non_identical_objects=True)

            for ppsth, stid in zip(psths, stids):
                t_start = ppsth[0].t_start
                duration = ppsth[0].t_stop-ppsth[0].t_start
                al = []
                for n in self.parameters.neurons:
                    ac = numpy.correlate(numpy.array(ppsth[:, n]),
                                         numpy.array(ppsth[:, n]),
                                         mode='full')
                    div = numpy.sum(numpy.power(numpy.array(ppsth[:, n]), 2))
                    if div != 0:
                        ac = ac / div
                    al.append(
                        AnalogSignal(ac,
                                     t_start=-duration+self.parameters.bin_length*t_start.units/2,
                                     sampling_period=self.parameters.bin_length*qt.ms,
                                     units=qt.dimensionless))

                logger.debug('Adding AnalogSignalList:' + str(sheet))
                self.datastore.full_datastore.add_analysis_result(
                    AnalogSignalList(al,
                                     self.parameters.neurons,
                                     qt.ms,
                                     qt.dimensionless,
                                     x_axis_name='time',
                                     y_axis_name='autocorrelation',
                                     sheet_name=sheet,
                                     tags=self.tags,
                                     analysis_algorithm=self.__class__.__name__,
                                     stimulus_id=str(stid)))


class ModulationRatio(Analysis):
    """
    This analysis calculates the modulation ration (as the F1/F0) for all
    neurons in the data using all available responses recorded to the
    FullfieldDriftingSinusoidalGrating stimuli. This method also requires
    that AveragedOrientationTuning has already been calculated.
    """

    required_parameters = ParameterSet({
      'bin_length': float,  # (ms) the size of bin to construct the PSTH from
    })

    def perform_analysis(self):
        for sheet in self.datastore.sheets():
            # Load up spike trains for the right sheet and the corresponding
            # stimuli, and transform spike trains into psth
            dsv = queries.param_filter_query(self.datastore, sheet_name=sheet,st_name='FullfieldDriftingSinusoidalGrating')
            assert queries.equal_stimulus_type(dsv)

            psths = [psth(seg.spiketrains, self.parameters.bin_length)
                     for seg in dsv.get_segments()]
            st = [MozaikParametrized.idd(s) for s in dsv.get_stimuli()]

            # average across trials
            psths, stids = colapse(psths,
                                   st,
                                   parameter_list=['trial'],
                                   func=neo_mean,
                                   allow_non_identical_objects=True)

            # retrieve the computed orientation preferences
            pnvs = self.datastore.get_analysis_result(identifier='PerNeuronValue',
                                                      sheet_name=sheet,
                                                      value_name='orientation preference')
            if len(pnvs) != 1:
                logger.error("ERROR: Expected only one PerNeuronValue per sheet "
                             "with value_name 'orientation preference' in datastore, got: "
                             + str(len(pnvs)))
                return None
        
            or_pref = pnvs[0]

            # find closest orientation of grating to a given orientation preference of a neuron
            # first find all the different presented stimuli:
            ps = {}
            for s in st:
                ps[MozaikParametrized.idd(s).orientation] = True
            ps = ps.keys()

            # now find the closest presented orientations
            closest_presented_orientation = []
            for i in xrange(0, len(or_pref.values)):
                circ_d = 100000
                idx = 0
                for j in xrange(0, len(ps)):
                    if circ_d > circular_dist(or_pref.values[i], ps[j], numpy.pi):
                        circ_d = circular_dist(or_pref.values[i], ps[j], numpy.pi)
                        idx = j
                closest_presented_orientation.append(ps[idx])

            closest_presented_orientation = numpy.array(closest_presented_orientation)

            # collapse along orientation - we will calculate MR for each
            # parameter combination other than orientation
            d = colapse_to_dictionary(psths, stids, "orientation")
            for (st, vl) in d.items():
                # here we will store the modulation ratios, one per each neuron
                modulation_ratio = numpy.zeros((numpy.shape(psths[0])[1],))
                frequency = MozaikParametrized.idd(st).temporal_frequency * MozaikParametrized.idd(st).params()['temporal_frequency'].units
                for (orr, ppsth) in zip(vl[0], vl[1]):
                    for j in numpy.nonzero(orr == closest_presented_orientation)[0]:
                        modulation_ratio[j] = self.calculate_MR(ppsth[:, j],
                                                                frequency)
                logger.debug('Adding PerNeuronValue:' + str(sheet))
                self.datastore.full_datastore.add_analysis_result(
                    PerNeuronValue(modulation_ratio,
                                   qt.dimensionless,
                                   value_name='Modulation ratio',
                                   sheet_name=sheet,
                                   tags=self.tags,
                                   period=None,
                                   analysis_algorithm=self.__class__.__name__,
                                   stimulus_id=str(st)))

            import pylab
            pylab.figure()
            pylab.hist(modulation_ratio)

    def calculate_MR(signal, frequency):
        """
        Calculates MR at frequency 1/period for each of the signals in the signal_list

        Returns an array of MRs on per each signal in signal_list
        """
        duration = signal.t_stop - signal.t_start
        period = 1/frequency
        period = period.rescale(signal.t_start.units)
        cycles = duration / period
        first_har = round(cycles)

        fft = numpy.fft.fft(signal)

        if abs(fft[0]) != 0:
            return 2 *abs(fft[first_har]) /abs(fft[0])
        else:
            return 0

class Conductance_F0andF1(Analysis):
      """
      Calculates the DC and first harmonic of trial averaged conductances for each neuron
      measured to FullfieldDriftingSinusoidalGrating. 
      Stores them in PerNeuronValue datastructures (one for exc. one for inh. conductances).
      Note that only neurons for which conductances were measured will be stored.
      
      DSV has to only contain 
      """

      def perform_analysis(self):
            dsv1 = queries.param_filter_query(self.datastore,st_name='FullfieldDriftingSinusoidalGrating')
            for sheet in dsv1.sheets():
                dsv = queries.param_filter_query(dsv1, sheet_name=sheet)
                segs, stids = colapse(dsv.get_segments(),dsv.get_stimuli(),parameter_list=['trial'],allow_non_identical_objects=True)
                for segs,st in zip(segs, stids):
                    first_analog_signal = segs[0].get_esyn(segs[0].get_stored_esyn_ids()[0],idd=True)
                    duration = first_analog_signal.t_stop - first_analog_signal.t_start
                    frequency = MozaikParametrized.idd(st).temporal_frequency * MozaikParametrized.idd(st).params()['temporal_frequency'].units
                    period = 1/frequency
                    period = period.rescale(first_analog_signal.t_start.units)
                    cycles = duration / period
                    first_har = round(cycles)
                    idd = segs[0].get_stored_esyn_ids()[0]
                    e_f0 = [abs(numpy.fft.fft(numpy.mean([seg.get_esyn(idd,idd=True) for seg in segs],axis=0))[0]) for idd in segs[0].get_stored_esyn_ids()]
                    i_f0 = [abs(numpy.fft.fft(numpy.mean([seg.get_isyn(idd,idd=True) for seg in segs],axis=0))[0]) for idd in segs[0].get_stored_isyn_ids()]
                    e_f1 = [2*abs(numpy.fft.fft(numpy.mean([seg.get_esyn(idd,idd=True) for seg in segs],axis=0))[first_har]) for idd in segs[0].get_stored_esyn_ids()]
                    i_f1 = [2*abs(numpy.fft.fft(numpy.mean([seg.get_isyn(idd,idd=True) for seg in segs],axis=0))[first_har]) for idd in segs[0].get_stored_isyn_ids()]
                    self.datastore.full_datastore.add_analysis_result(PerNeuronValue(e_f0,first_analog_signal.units,value_name = 'F0_Exc_Cond',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    self.datastore.full_datastore.add_analysis_result(PerNeuronValue(i_f0,first_analog_signal.units,value_name = 'F0_Inh_Cond',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    self.datastore.full_datastore.add_analysis_result(PerNeuronValue(e_f1,first_analog_signal.units,value_name = 'F1_Exc_Cond',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    self.datastore.full_datastore.add_analysis_result(PerNeuronValue(i_f1,first_analog_signal.units,value_name = 'F1_Inh_Cond',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        

class TrialVariability(Analysis):
      """
      For each stimulus and neuron it calculates the average trial-to-trial variability of Vm or conductance (depending on parameters). 
      It stores each in a PerNeuronValue structure, one per sheet and variable.
      """
      required_parameters = ParameterSet({
        'vm': bool,  # calculate variability for Vm?
        'cond_exc': bool,  # calculate variability for excitatory conductance?
        'cond_inh': bool,  # calculate variability for inhibitory conductance?
      })
      
      
      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
                segs, stids = colapse(dsv.get_segments(),dsv.get_stimuli(),parameter_list=['trial'],allow_non_identical_objects=True)
                for segs,st in zip(segs,stids):
                    if self.parameters.vm:
                        vm = [numpy.mean(numpy.var(numpy.array([s.get_vm(i,idd=True) for s in segs]),axis=0)) for i in segs[0].get_stored_v_ids()]
                        self.datastore.full_datastore.add_analysis_result(PerNeuronValue(vm,segs[0].get_vm(segs[0].get_stored_v_ids()[0],idd=True).units,value_name = 'VM Variance',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_exc:                        
                        cond_exc = [numpy.mean(numpy.var(numpy.array([s.get_esyn(i,idd=True) for s in segs]),axis=0)) for i in segs[0].get_stored_esyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(PerNeuronValue(cond_exc,segs[0].get_esyn(segs[0].get_stored_esyn_ids()[0],idd=True).units,value_name = 'Exc. Cond. Variance',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_inh:                                                
                        cond_inh = [numpy.mean(numpy.var(numpy.array([s.get_isyn(i,idd=True) for s in segs]),axis=0)) for i in segs[0].get_stored_isyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(PerNeuronValue(cond_inh,segs[0].get_isyn(segs[0].get_stored_isyn_ids()[0],idd=True).units,value_name = 'Inh. Cond. Variance',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    

class TrialMean(Analysis):
      """
      For each stimulus and neuron it calculates the average trial-to-trial mean of Vm or conductance (depending on parameters). 
      It stores each in a PerNeuronValue structure, one per sheet and variable.
      """
      required_parameters = ParameterSet({
        'vm': bool,  # calculate variability for Vm?
        'cond_exc': bool,  # calculate variability for excitatory conductance?
        'cond_inh': bool,  # calculate variability for inhibitory conductance?
      })
      
      
      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
                # colapse by trials 
                segs, stids = colapse(dsv.get_segments(),dsv.get_stimuli(),parameter_list=['trial'],allow_non_identical_objects=True)
                for segs,st in zip(segs,stids):
                    if self.parameters.vm:
                        vm = [numpy.mean(numpy.array([s.get_vm(i,idd=True) for s in segs])) for i in segs[0].get_stored_v_ids()]
                        self.datastore.full_datastore.add_analysis_result(PerNeuronValue(vm,segs[0].get_vm(segs[0].get_stored_v_ids()[0],idd=True).units,value_name = 'VM Mean',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_exc:                        
                        cond_exc = [numpy.mean(numpy.array([s.get_esyn(i,idd=True) for s in segs])) for i in segs[0].get_stored_esyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(PerNeuronValue(cond_exc,segs[0].get_esyn(segs[0].get_stored_esyn_ids()[0],idd=True).units,value_name = 'Exc. Cond. Mean',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
                    if self.parameters.cond_inh:                                                
                        cond_inh = [numpy.mean(numpy.array([s.get_isyn(i,idd=True) for s in segs])) for i in segs[0].get_stored_isyn_ids()]
                        self.datastore.full_datastore.add_analysis_result(PerNeuronValue(cond_inh,segs[0].get_isyn(segs[0].get_stored_isyn_ids()[0],idd=True).units,value_name = 'Inh. Cond. Mean',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        

class GaussianTuningCurveFit(Analysis):
      """
      Fits each tuning curve with a gaussian.
      
      It takes a dsv containing some PerNeuronValues.
      All PerNeuronValues have to belong to stimuli of the same type and
      contain the same type of values (i.e. have the same `value_name`).

      For each combination of parameters of the stimuli other than `parameter_name`
      `GaussianTuningCurveFit` fits a gaussian tuning curve and stores the fitted 
      parameters in a 
    
      """   
      required_parameters = ParameterSet({
          'parameter_name': str  # the parameter_name through which to fit the tuning curve
      })      
      
      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                dsv = queries.param_filter_query(self.datastore,identifier='PerNeuronValue',sheet_name=sheet)
                if len(dsv.get_analysis_result()) == 0: continue
                assert queries.equal_ads_except(dsv, ['stimulus_id'])
                assert queries.ads_with_equal_stimulus_type(dsv)
                self.pnvs = dsv.get_analysis_result()
                
                # get stimuli
                self.st = [MozaikParametrized.idd(s.stimulus_id) for s in self.pnvs]
                
                # check that it is periodic 
                period = self.st[0].params()[self.parameters.parameter_name].period
                
                # transform the pnvs into a dictionary of tuning curves according along the parameter_name
                self.tc_dict = colapse_to_dictionary([z.values for z in self.pnvs],self.st,self.parameters.parameter_name)
                for k in self.tc_dict.keys():
                        z = []
                        for i in xrange(0,len(self.pnvs[0].values)):
                        #for i in xrange(0,20):
                            res = self.fitgaussian(self.tc_dict[k][0],[a[i] for a in self.tc_dict[k][1]],period)
                            if res == None:
                               logger.debug('Failed to fit tuning curve %s for neuron %d' % (k,i))
                               return
                            z.append(res)    
                        #import pylab
                        #pylab.show()
                        res = numpy.array(z)
                        if res != None:
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,0],self.pnvs[0].value_units,value_name = self.parameters.parameter_name + ' baseline',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,1],self.pnvs[0].value_units,value_name = self.parameters.parameter_name + ' max',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,2],MozaikParametrized.idd(self.st[0]).params()[self.parameters.parameter_name].units,value_name = self.parameters.parameter_name + ' selectivity',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(res[:,3],MozaikParametrized.idd(self.st[0]).params()[self.parameters.parameter_name].units,value_name = self.parameters.parameter_name + ' preference',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                           self.datastore.full_datastore.add_analysis_result(PerNeuronValue(180*res[:,2]/numpy.pi*2.35482,MozaikParametrized.idd(self.st[0]).params()[self.parameters.parameter_name].units,value_name = self.parameters.parameter_name + ' HWHH',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(k)))        
                        else:
                           logger.debug('Failed to fit tuning curve %s for neuron %d' % (k,i))
                            
                    
       
      def fitgaussian(self,X,Y,period):
          if period != None:
            fitfunc = lambda p,x:  p[0] + p[1]*numpy.exp(-circular_dist(p[3],x,period)**2/(2*p[2]**2))
          else:
            fitfunc = lambda p,x:  p[0] + p[1]*numpy.exp(-numpy.abs(p[3]-x)**2/(2*p[2]**2))  
          errfunc = lambda p, x, y: fitfunc(p,x) - y # Distance to the target function
          
          p0 = [0, 1.0, 0.5,0.0] # Initial guess for the parameters
          p0[0] = numpy.min(Y)
          if period != None:
            p0[3] = circ_mean(numpy.array([X]),weights=numpy.array([Y]),axis=1,low=0,high=period,normalize=True)[0]
          else:
            p0[3] = numpy.average(numpy.array(X),weights=numpy.array(Y))[0]
            
          p1, success = scipy.optimize.leastsq(errfunc, p0[:], args=(X,Y))      
          
          # if the fit is very bad - error greater than 30% of the Y magnitude
          if numpy.linalg.norm(fitfunc(p1,X)-Y)/numpy.linalg.norm(Y) > 0.1:
             p1 = [0,0,0,0]
                    
          if success:
            return p1
          else :
            return [0,0,0,0]
                
class CV_ISI(Analysis):
      """
      A wrapper over NeuroTools's cv_isi. 
      
      The datastore should contain 
      """

      def perform_analysis(self):
            for sheet in self.datastore.sheets():
                # Load up spike trains for the right sheet and the corresponding stimuli, and
                # transform spike trains into psth
                dsv = queries.param_filter_query(self.datastore, sheet_name=sheet)
                assert equal_ads_except(dsv,['stimulus_id'])
                for seg,st in zip(dsv.get_segments(),dsv.get_stimuli()):
                    isi = seg.cv_isi()
                    self.datastore.full_datastore.add_analysis_result(PerNeuronValue(isi,qt.dimensionless,value_name = 'CV of ISI',sheet_name=sheet,tags=self.tags,period=None,analysis_algorithm=self.__class__.__name__,stimulus_id=str(st)))        
